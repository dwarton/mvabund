<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="Fits a generalised linear model with a LASSO penalty, using an iteratively reweighted local linearisation approach, given a value of the penalty parameter (lamb). Can handle negative binomial family, even with overdispersion parameter unknown, as well as other GLM families."><title>Fits a Generalised Linear Models with a LASSO (or L1) penalty, given a value of the penalty parameter. — glm1 • mvabund</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Fits a Generalised Linear Models with a LASSO (or L1) penalty, given a value of the penalty parameter. — glm1"><meta property="og:description" content="Fits a generalised linear model with a LASSO penalty, using an iteratively reweighted local linearisation approach, given a value of the penalty parameter (lamb). Can handle negative binomial family, even with overdispersion parameter unknown, as well as other GLM families."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">mvabund</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">4.2.2</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item">
  <a class="nav-link" href="../articles/mvabund.html">Get started</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Functions glossary</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/Offsets.html">Using offsets with count data</a>
  </div>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/aliceyiwang/mvabund">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Fits a Generalised Linear Models with a LASSO (or L1) penalty, given a value of the penalty parameter.</h1>
      
      <div class="d-none name"><code>glm1.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Fits a generalised linear model with a LASSO penalty, using an iteratively reweighted local linearisation approach, given a value of the penalty parameter (lamb). Can handle negative binomial family, even with overdispersion parameter unknown, as well as other GLM families.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">glm1</span><span class="op">(</span><span class="va">y</span>, <span class="va">X</span>, <span class="va">lambda</span>, family <span class="op">=</span> <span class="st">"negative.binomial"</span>, weights <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>,</span>
<span>     b.init <span class="op">=</span> <span class="cn">NA</span>, phi.init <span class="op">=</span> <span class="cn">NA</span>, phi.method <span class="op">=</span> <span class="st">"ML"</span>, tol <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1e-08</span>, <span class="va">.Machine</span><span class="op">$</span><span class="va">double.eps</span><span class="op">)</span>,</span>
<span>     n.iter <span class="op">=</span> <span class="fl">100</span>, phi.iter <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>y</dt>
<dd><p>A vector of values for the response variable.</p></dd>

  <dt>X</dt>
<dd><p>A design matrix of p explanatory variables.</p></dd>

  <dt>family</dt>
<dd><p>The family of the response variable, see <code><a href="https://rdrr.io/r/stats/family.html" class="external-link">family</a></code>. Negative binomial with unknown overdispersion can be specified as "negative.binomial", and is the default.</p></dd>

  <dt>lambda</dt>
<dd><p>The penalty parameter applied to slope parameters. Different penalties can be specified for different parameters by specifying lamb as a vector, whose length is the number of columns of X. If scalar, this penalty is applied uniformly across all parameters except for the first (assuming that it is an intercept)</p></dd>

  <dt>weights</dt>
<dd><p>Observation weights. These might be useful if you want to fit a Poisson point process model...</p></dd>

  <dt>b.init</dt>
<dd><p>Initial slope estimate. Must be a vector of the same length as the number of columns in X.</p></dd>

  <dt>phi.init</dt>
<dd><p>Initial estimate of the negative binomial overdispersion parameter. Must be scalar.</p></dd>

  <dt>phi.method</dt>
<dd><p>Method of estimating overdispersion.</p></dd>

  <dt>tol</dt>
<dd><p>A vector of two values, specifying convergence tolerance, and the value to truncate fitted values at.</p></dd>

  <dt>n.iter</dt>
<dd><p>Number of iterations to attempt before bailing.</p></dd>

  <dt>phi.iter</dt>
<dd><p>Number of iterations estimating the negative binomial overdispersion parameter (if applicable) before returning to slope estimation. Default is one step, i.e. iterating between one-step estimates of beta and phi.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>This function fits a generalised linear model with a LASSO penalty, sometimes referred to as an L1 penalty or L1 norm, hence the name glm1. The model is fit using a local linearisation approach as in Osborne et al (2000), nested inside iteratively reweighted (penalised) least squares. Look it's not the fastest thing going around, try <code>glmnet</code> if you want something faster (and possibly rougher as an approximation). The main advantage of the <code>glm1</code> function is that it has been written to accept any glm family argument (although not yet tested beyond discrete data!), and also the negative binomial distribution, which is especially useful for modelling overdispersed counts.</p>
<p>For negative binomial with unknown overdispersion use <code>"negative.binomial"</code>, or if overdispersion is to be specified, use <code>negative.binomial(theta)</code> as in the <code>MASS</code> package. Note that the output refers to phi=1/theta, i.e. the overdispersion is parameterised such that the variance is mu+phi*mu^2. Hence values of phi close to zero suggest little overdispersion, values over one suggest a lot.</p>
    </div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <dl><dt>coefficients</dt>
<dd><p>Vector of parameter estimates</p></dd>

<dt>fitted.values</dt>
<dd><p>Vector of predicted values (on scale of the original response)</p></dd>

<dt>logLs</dt>
<dd><p>Vector of log-likelihoods at each iteration of the model.  The last entry is the log-likelihood for the final fit.</p></dd>

<dt>phis</dt>
<dd><p>Estimated overdispersion parameter at each iteration, for a negative binomial fit.</p></dd>

<dt>phi</dt>
<dd><p>Final estimate of the overdispersion parameter, for a negative binomial fit.</p></dd>

<dt>score</dt>
<dd><p>Vector of score equation values for each parameter in the model.</p></dd>

<dt>counter</dt>
<dd><p>Number of iterations until convergence. Set to Inf for a model that didn't converge.</p></dd>

<dt>check</dt>
<dd><p>Logical for whether the Kuhn-KArush-Tucker conditions are saitsfied.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Osborne, M.R., Presnell, B. and Turlach, B.A. (2000) On the LASSO and its dual. Journal of Computational and Graphical Statistics, 9, 319-337.</p>
    </div>
    <div class="section level2">
    <h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a></h2>
    <p>David I. Warton &lt;David.Warton@unsw.edu.au&gt;, Ian W. Renner and Luke Wilson.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p><code><a href="glm1path.html">glm1path</a></code>, <code>glm1</code>, <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></code>, <code><a href="https://rdrr.io/r/stats/family.html" class="external-link">family</a></code></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">spider</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">Alopacce</span> <span class="op">&lt;-</span> <span class="va">spider</span><span class="op">$</span><span class="va">abund</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span></span></span>
<span class="r-in"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html" class="external-link">model.matrix</a></span><span class="op">(</span><span class="op">~</span><span class="va">.</span>,data<span class="op">=</span><span class="va">spider</span><span class="op">$</span><span class="va">x</span><span class="op">)</span> <span class="co"># to get design matrix with intercept term</span></span></span>
<span class="r-in"><span><span class="co">#fit a LASSO-penalised negative binomial GLM, with penalty parameter 10:</span></span></span>
<span class="r-in"><span><span class="va">ft</span> <span class="op">=</span> <span class="fu">glm1</span><span class="op">(</span><span class="va">Alopacce</span>,<span class="va">X</span>,lambda<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu"><a href="mvabund-internal.html">plot</a></span><span class="op">(</span><span class="va">ft</span><span class="op">$</span><span class="va">logLs</span><span class="op">)</span> <span class="co"># a plot of the log-likelihood, each iteration to convergence</span></span></span>
<span class="r-plt img"><img src="glm1-1.png" alt="" width="700" height="433"></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">ft</span><span class="op">)</span> <span class="co"># coefficients in the final model</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   (Intercept)      soil.dry     bare.sand fallen.leaves          moss </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   -0.15536720    0.00000000    0.00000000   -0.37292658    0.07882227 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>    herb.layer    reflection </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>    0.00000000    0.58721954 </span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Yi Wang, Ulrike Naumann, Dirk Eddelbuettel, John Wilshire, David Warton.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer></div>

  

  

  </body></html>

